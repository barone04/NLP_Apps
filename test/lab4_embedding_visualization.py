import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import numpy as np
from src.representations.word_embedder import WordEmbedder


def visualize_embeddings(model_name="glove-wiki-gigaword-50", method="pca"):
    print(f"üîπ Loading model: {model_name} ...")
    embedder = WordEmbedder(model_name)

    # Ch·ªçn m·ªôt s·ªë t·ª´ ti√™u bi·ªÉu (c√≥ th·ªÉ t√πy ch·ªânh)
    words = [
        "king", "queen", "man", "woman", "prince", "princess",
        "apple", "banana", "orange", "fruit",
        "car", "truck", "vehicle", "road",
        "dog", "cat", "animal", "pet"
    ]

    # L·∫•y vector c·ªßa t·ª´ng t·ª´
    vectors = []
    valid_words = []
    for word in words:
        vec = embedder.get_vector(word)
        if vec is not None:
            vectors.append(vec)
            valid_words.append(word)

    X = np.array(vectors)

    # Gi·∫£m chi·ªÅu
    if method.lower() == "pca":
        reducer = PCA(n_components=2)
    else:
        reducer = TSNE(n_components=2, perplexity=5, random_state=42)

    reduced = reducer.fit_transform(X)

    # V·∫Ω scatter plot
    plt.figure(figsize=(10, 8))
    plt.scatter(reduced[:, 0], reduced[:, 1], color="steelblue")

    # Ghi nh√£n t·ª´
    for i, word in enumerate(valid_words):
        plt.annotate(word, (reduced[i, 0] + 0.02, reduced[i, 1] + 0.02))

    plt.title(f"Word Embeddings Visualization ({method.upper()})")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    # Ch·∫°y th·ª≠ v·ªõi PCA
    # visualize_embeddings(method="pca")


     visualize_embeddings(method="tsne")
